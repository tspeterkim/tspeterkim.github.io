---
layout: post
comments: true
title:  'The One Billion Row Challenge in CUDA: from 17 minutes to 17 seconds'
date:   2024-04-10 4:50:23 -0700
categories: jekyll update
---
<style>
{% include blogposts.css %}
</style>

On my journey to learn CUDA, I decided to tackle the [One Billion Row Challenge](https://1brc.dev/) with it.

<img src="/images/cuda-1brc/1brc.png" width="500" style="margin: 0 auto; display: block; "/>

The challenge is simple, but implementing it in CUDA was not. Here I will share [my solution](https://github.com/tspeterkim/cuda-1brc/blob/main/fast.cu) that runs in 16.8 seconds 
on a [V100](https://aws.amazon.com/ec2/instance-types/p3/). It's certainly not the fastest solution, but it is the first
one of its kind (no [cudf](https://github.com/rapidsai/cudf), hand-written kernels only). I *challenge* other CUDA 
enthusiasts to make it faster.

## Baseline in pure C++

You can't improve what you don't measure. Since I'm going to be writing C++ anyways for CUDA, let's use a C++ baseline.
My CUDA code should be faster than this.

The approach is straight-forward: read the file line by line, parse it into the city and temperature, and accumulate
them in a [STL map](https://cplusplus.com/reference/map/map/).

{% highlight cpp %}
while (getline(file, line)) {
    istringstream iss(line);
    string station;
    float temp;
    getline(iss, station, ';');
    iss >> temp;

    auto it = stationStats.find(station);
    if (it == stationStats.end()) {
        stationStats[station] = {temp, temp, temp, 1};
    } else {
        Stat& s = it->second;
        s.min = min(s.min, temp);
        s.max = max(s.max, temp);
        s.sum += temp;
        s.count++;
    }
}

ofstream measurements("measurements.out");
for (auto& pair : stationStats) {
    const Stat& s = pair.second;
    float mean = s.sum / s.count;
    measurements << pair.first << "=" << s.min << "/";
    measurements << fixed << setprecision(1) << mean << "/";
    measurements << s.max << endl;
}
{% endhighlight %}

**This runs in 16.5 minutes.** Let's improve this.

## Work Partitioning Approach

### One Billion Threads?

The whole promise of CUDA and other parallel programming APIs is that you can parallelize your workload across many
processes. For CUDA, it's a **SIMT** model - a **s**ingle **i**nstruction is executed across **m**ultiple **t**hreads in
parallel.

Great, so let's just use one billion threads to process one billion lines concurrently!

Unfortunately, we can't *just* launch one billion threads. We first need to prepare each line buffer for each thread 
to process. However, preparing these one billion line buffers requires reading the entire file, line by line (unless 
the lines were already placed in one billion files, but that would make this the One Billion Files Challenge). 
Thus, the effort involved in setting up these buffers would essentially replicate the baseline workload, making this
approach counterproductive.

### Use Byte Offsets

The solution is to prepare file offsets instead of line buffers. These offsets are obtained iteratively, stepping 
through the entire file buffer by the desired split size (= total file size / desired number of parts), and marking the
position of a new line character:

{% highlight cpp %}
long long split_size = size / num_parts;
long long offset = 0;
std::vector<Part> parts;
while (offset < size) {
    long long seek_offset = std::max(offset + split_size - MAX_CITY_BYTE, 0LL);
    if (seek_offset > size) {
        parts.back().length += size-offset;
        break;
    }
    file.seekg(seek_offset, std::ios::beg);
    char buf[MAX_CITY_BYTE];
    file.read(buf, MAX_CITY_BYTE);

    std::streamsize n = file.gcount();
    std::streamsize newline = -1;
    for (int i = n - 1; i >= 0; --i) {
        if (buf[i] == '\n') {
            newline = i;
            break;
        }
    }
    int remaining = n - newline - 1;
    long long next_offset = seek_offset + n - remaining;
    parts.push_back({offset, next_offset-offset});
    offset = next_offset;
}
{% endhighlight %}

This is much faster than reading the entire file because we are working with integer byte values. For example, say we 
want to partition the 14GB input file for two threads. The while loop iterates twice (offset = 0GB -> 7GB -> 14GB). 
Contrast this to a line-based approach. We would need to iterate 500M (= 1B / 2) times to load 500M lines into our 
first partition.

In reality, we need more than just two threads. But one billion is too much.
Not because our GPU can't handle one billion threads, but because finding one billion offsets becomes 
the bottleneck in our overall runtime. We haven't even gotten to launching our CUDA kernels. 
We need to minimize our preparation time as much as possible.

**For my solution, I ended up creating 1M partitions that took 2.6 seconds out of the entire 16.8 seconds.**
(In comparison, creating 100M partitions alone takes over 3 minutes.)

## CUDA Kernel

The rest of the time is (finally!) spent in the CUDA kernel. The idea behind it is simple. 
Each thread indexes into a different part of the file buffer, parses it to get the cities and temperatures, and 
updates the min/max/avg statistics. 

The implementation, however, is not trivial due to the following reasons (in order of annoyance):
* CUDA's `AtomicMin` and `AtomicMax` only work with int values. We'll make our own float-value-accepting variants.
* No `std::string` in CUDA. Time to make our own `atof`, `strcmp`, `getline`. Get ready for null terminators (`'\0'`).
* No `std::map` either. How can we pass in the *city string to array index* lookup table into our CUDA kernel?

Let's go through these one by one.

### AtomicMin & AtomicMax for Floats

### C Strings

### City Index Lookup with Pre-sorted Cities + Binary Search

## Conclusion

{% include disqus.html %}
